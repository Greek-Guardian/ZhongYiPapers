{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def save_to(entry, filename = r'/Users/lzd/Desktop/workspace/ZhongYiPapers/papers_lzd.json'):\n",
    "    try:\n",
    "        # utf-8将一个汉字编码为3个字节,gbk将一个汉字编码为2个字节,\n",
    "        with open(filename, 'w',encoding='utf-8') as new_f:  # 重新写入\n",
    "            json.dump(entry, new_f,ensure_ascii=False,indent=4) # 最后一个参数会保证dump之后的结果所有的字符都能被ascii表示\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        print('Make sure the json file is in valid format,[{},{}...]')\n",
    "\n",
    "def open_json(filename = r'/Users/lzd/Desktop/workspace/ZhongYiPapers/papers_lzd.json'):\n",
    "    with open(filename, 'r',encoding='utf-8') as f:\n",
    "        existing_data = json.load(f)\n",
    "        return existing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "papers = open_json()\n",
    "print(len(papers))\n",
    "papers_numed_authers = []\n",
    "papers_unnumed_authers = []\n",
    "papers_item_missing = []\n",
    "provinces = [\"河北\",\"山西\",\"黑龙江\",\"吉林\",\"辽宁\",\"江苏\",\"浙江\",\"安徽\",\"福建\",\"江西\",\"山东\",\"河南\",\"湖北\",\"湖南\",\"广东\",\"海南\",\"四川\",\"贵州\",\"云南\",\"陕西\",\"甘肃\",\"青海\",\"台湾\",\"内蒙古\",\"广西\",\"西藏\",\"宁夏\",\"新疆\",\"北京\",\"天津\",\"上海\",\"重庆\",\"香港\",\"澳门\"]\n",
    "\n",
    "cities = {\n",
    "    \"北京\":[\n",
    "        \"北京\"\n",
    "    ],\n",
    "    \"上海\":[\n",
    "        \"上海\"\n",
    "    ],\n",
    "    \"重庆\":[\n",
    "        \"重庆\",\"西南政法大学\"\n",
    "    ],\n",
    "    \"天津\":[\n",
    "        \"天津\"\n",
    "    ],\n",
    "    \"河北\":[\n",
    "        \"河北\",\"石家庄\",\"唐山\",\"秦皇岛\",\"邯郸\",\"邢台\",\"保定\",\"张家口\",\"承德\",\"沧州\",\"廊坊\",\"衡水\"\n",
    "    ],\n",
    "    \"山西\":[\n",
    "        \"山西\",\"太原\",\"大同\",\"阳泉\",\"长治\",\"晋城\",\"朔州\",\"晋中\",\"运城\",\"忻州\",\"临汾\",\"吕梁\"\n",
    "    ],\n",
    "    \"内蒙古\":[\n",
    "        \"内蒙古\",\"呼和浩特\",\"包头\",\"乌海\",\"赤峰\",\"通辽\",\"鄂尔多斯\",\"呼伦贝尔\",\"巴彦淖尔\",\"乌兰察布\",\"兴安盟\",\"锡林郭勒盟\",\"伊克昭盟\",\"阿拉善\"\n",
    "    ],\n",
    "    \"辽宁\":[\n",
    "        \"辽宁\",\"沈阳\",\"大连\",\"鞍山\",\"抚顺\",\"本溪\",\"丹东\",\"锦州\",\"营口\",\"阜新\",\"辽阳\",\"盘锦\",\"铁岭\",\"朝阳\",\"葫芦岛\"\n",
    "    ],\n",
    "    \"吉林\":[\n",
    "        \"吉林\",\"长春\",\"吉林\",\"四平\",\"辽源\",\"通化\",\"白山\",\"松原\",\"白城\",\"延边\"\n",
    "    ],\n",
    "    \"黑龙江\":[\n",
    "        \"黑龙江\",\"哈尔滨\",\"齐齐哈尔\",\"鸡西\",\"鹤岗\",\"双鸭山\",\"大庆\",\"伊春\",\"佳木斯\",\"七台河\",\"牡丹江\",\"黑河\",\"绥化\",\"大兴安岭\"\n",
    "    ],\n",
    "    \"江苏\":[\n",
    "        \"江苏\",\"南京\",\"无锡\",\"徐州\",\"常州\",\"苏州\",\"南通\",\"连云港\",\"淮安\",\"盐城\",\"扬州\",\"镇江\",\"泰州\",\"宿迁\",\"淮阴\"\n",
    "    ],\n",
    "    \"浙江\":[\n",
    "        \"浙江\",\"杭州\",\"宁波\",\"温州\",\"嘉兴\",\"湖州\",\"绍兴\",\"金华\",\"衢州\",\"舟山\",\"台州\",\"丽水\"\n",
    "    ],\n",
    "    \"安徽\":[\n",
    "        \"安徽\",\"合肥\",\"芜湖\",\"蚌埠\",\"淮南\",\"马鞍山\",\"淮北\",\"铜陵\",\"安庆\",\"黄山\",\"阜阳\",\"宿州\",\"滁州\",\"六安\",\"宣城\",\"池州\",\"亳州\"\n",
    "    ],\n",
    "    \"福建\":[\n",
    "        \"福建\",\"福州\",\"厦门\",\"莆田\",\"三明\",\"泉州\",\"漳州\",\"南平\",\"龙岩\",\"宁德\"\n",
    "    ],\n",
    "    \"江西\":[\n",
    "        \"江西\",\"南昌\",\"景德镇\",\"萍乡\",\"九江\",\"抚州\",\"鹰潭\",\"赣州\",\"吉安\",\"宜春\",\"新余\",\"上饶\",\"赣南\"\n",
    "    ],\n",
    "    \"山东\":[\n",
    "        \"山东\",\"济南\",\"青岛\",\"淄博\",\"枣庄\",\"东营\",\"烟台\",\"潍坊\",\"济宁\",\"泰安\",\"威海\",\"日照\",\"临沂\",\"德州\",\"聊城\",\"滨州\",\"菏泽\"\n",
    "    ],\n",
    "    \"河南\":[\n",
    "        \"河南\",\"郑州\",\"开封\",\"洛阳\",\"平顶山\",\"安阳\",\"鹤壁\",\"新乡\",\"焦作\",\"濮阳\",\"许昌\",\"漯河\",\"三门峡\",\"南阳\",\"商丘\",\"信阳\",\"周口\",\"驻马店\",\"济源\"\n",
    "    ],\n",
    "    \"湖北\":[\n",
    "        \"湖北\",\"武汉\",\"黄石\",\"十堰\",\"宜昌\",\"襄阳\",\"鄂州\",\"荆门\",\"孝感\",\"荆州\",\"黄冈\",\"咸宁\",\"随州\",\"恩施\",\"仙桃\",\"潜江\",\"天门\",\"神农架\"\n",
    "    ],\n",
    "    \"湖南\":[\n",
    "        \"湖南\",\"长沙\",\"株洲\",\"湘潭\",\"衡阳\",\"邵阳\",\"岳阳\",\"常德\",\"张家界\",\"益阳\",\"郴州\",\"永州\",\"怀化\",\"娄底\",\"湘西\"\n",
    "    ],\n",
    "    \"广东\":[\n",
    "        \"广东\",\"广州\",\"韶关\",\"深圳\",\"珠海\",\"汕头\",\"佛山\",\"江门\",\"湛江\",\"茂名\",\"肇庆\",\"惠州\",\"梅州\",\"汕尾\",\"河源\",\"阳江\",\"清远\",\"东莞\",\"中山\",\"潮州\",\"揭阳\",\"云浮\"\n",
    "    ],\n",
    "    \"广西\":[\n",
    "        \"广西\",\"南宁\",\"柳州\",\"桂林\",\"梧州\",\"北海\",\"防城港\",\"钦州\",\"贵港\",\"玉林\",\"百色\",\"贺州\",\"河池\",\"来宾\",\"崇左\"\n",
    "    ],\n",
    "    \"海南\":[\n",
    "        \"海南\",\"海口\",\"三亚\",\"三沙\",\"儋州\",\"琼海\",\"五指山\",\"文昌\",\"万宁\",\"东方\",\"定安\",\"屯昌\",\"澄迈\",\"临高\",\"白沙\",\"昌江\",\"乐东\",\"陵水\",\"保亭\",\"琼中\"\n",
    "    ],\n",
    "    \"四川\":[\n",
    "        \"四川\",\"成都\",\"自贡\",\"攀枝花\",\"泸州\",\"德阳\",\"绵阳\",\"广元\",\"遂宁\",\"内江\",\"乐山\",\"南充\",\"眉山\",\"宜宾\",\"广安\",\"达州\",\"雅安\",\"巴中\",\"资阳\",\"阿坝\",\"甘孜\",\"凉山\",\"巴中\",\"眉山\",\"资阳\"\n",
    "    ],\n",
    "    \"贵州\":[\n",
    "        \"贵州\",\"贵阳\",\"六盘水\",\"遵义\",\"安顺\",\"毕节\",\"铜仁\",\"黔西南\",\"黔东南\",\"安顺\"\n",
    "    ],\n",
    "    \"云南\":[\n",
    "        \"云南\",\"昆明\",\"曲靖\",\"玉溪\",\"保山\",\"昭通\",\"丽江\",\"普洱\",\"临沧\",\"楚雄\",\"红河哈尼\",\"文山\",\"思茅\",\"西双版纳\",\"大理\",\"德宏\",\"怒江\"\n",
    "    ],\n",
    "    \"西藏\":[\n",
    "        \"西藏\",\"拉萨\",\"日喀则\",\"昌都\",\"林芝\",\"山南\",\"那曲\",\"阿里\"\n",
    "    ],\n",
    "    \"陕西\":[\n",
    "        \"陕西\",\"西安\",\"铜川\",\"宝鸡\",\"咸阳\",\"渭南\",\"延安\",\"汉中\",\"榆林\",\"安康\",\"商洛\"\n",
    "    ],\n",
    "    \"甘肃\":[\n",
    "        \"甘肃\",\"兰州\",\"嘉峪关\",\"金昌\",\"白银\",\"天水\",\"武威\",\"张掖\",\"平凉\",\"酒泉\",\"庆阳\",\"定西\",\"陇南\",\"临夏\",\"甘南\"\n",
    "    ],\n",
    "    \"青海\":[\n",
    "        \"青海\",\"西宁\",\"海东\",\"玉树\",\"果洛\",\"黄南\",\"海西\",\"海北\",\"海南藏族\"\n",
    "    ],\n",
    "    \"宁夏\":[\n",
    "        \"宁夏\",\"银川\",\"石嘴山\",\"吴忠\",\"固原\",\"中卫\"\n",
    "    ],\n",
    "    \"新疆\":[\n",
    "        \"新疆\",\"乌鲁木齐\",\"克拉玛依\",\"吐鲁番\",\"哈密\",\"昌吉\",\"博尔塔\",\"巴音郭楞\",\"阿克苏\",\"克孜勒苏柯尔克孜\",\"喀什\",\"和田\",\"伊犁\",\"塔城\",\"阿勒泰\",\"石河子\",\"阿拉尔\",\"图木舒克\",\"五家渠\"\n",
    "    ],\n",
    "    \"台湾\":[\n",
    "        \"台湾\",\"台北\",\"高雄\",\"基隆\",\"台中\",\"台南\",\"新北\",\"桃源\",\"宜兰\",\"新竹\",\"嘉义\"\n",
    "    ],\n",
    "    \"香港\":[\n",
    "        \"香港\"\n",
    "    ],\n",
    "    \"澳门\":[\n",
    "        \"澳门\"\n",
    "    ],\n",
    "}\n",
    "\n",
    "for paper in papers:\n",
    "    paper_province = []\n",
    "    for province in provinces:\n",
    "        for city in cities[province]:\n",
    "            if re.search(city, paper[\"affiliations\"])!=None:\n",
    "                paper_province.append(province)\n",
    "                break\n",
    "    paper[\"province\"] = paper_province\n",
    "    if paper[\"authors\"]==\"\" or paper[\"abstract\"]==\"\" or paper[\"affiliations\"]==\"\":\n",
    "        papers_item_missing.append(paper)\n",
    "        continue\n",
    "    if re.search(\"/d\", paper[\"authors\"])!=None:\n",
    "        papers_numed_authers.append(paper)\n",
    "        continue\n",
    "    else:\n",
    "        papers_unnumed_authers.append(paper)\n",
    "        continue\n",
    "print(len(papers_item_missing))\n",
    "print(len(papers_numed_authers))\n",
    "print(len(papers_unnumed_authers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in papers_unnumed_authers:\n",
    "    if item[\"province\"]!=[]:\n",
    "        continue\n",
    "    print((item[\"affiliations\"]))\n",
    "    print((item[\"province\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to(papers_item_missing, filename = r'/Users/lzd/Desktop/workspace/ZhongYiPapers/papers_item_missing.json')\n",
    "save_to(papers_numed_authers, filename = r'/Users/lzd/Desktop/workspace/ZhongYiPapers/papers_numed_authers.json')\n",
    "save_to(papers_unnumed_authers, filename = r'/Users/lzd/Desktop/workspace/ZhongYiPapers/papers_unnumed_authers.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = open_json()\n",
    "for paper in papers:\n",
    "    print(paper['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, os\n",
    "def save_to(entry, filename = r'/Users/lzd/Desktop/workspace/ZhongYiPapers/papers_lzd.json'):\n",
    "    try:\n",
    "        # utf-8将一个汉字编码为3个字节,gbk将一个汉字编码为2个字节,\n",
    "        with open(filename, 'w',encoding='utf-8') as new_f:  # 重新写入\n",
    "            json.dump(entry, new_f,ensure_ascii=False,indent=4) # 最后一个参数会保证dump之后的结果所有的字符都能被ascii表示\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        print('Make sure the json file is in valid format,[{},{}...]')\n",
    "\n",
    "def open_json(filename):\n",
    "    with open(filename, 'r',encoding='utf-8') as f:\n",
    "        existing_data = json.load(f)\n",
    "        return existing_data\n",
    "\n",
    "def corpus(corpus_path):\n",
    "    filenames = os.listdir(corpus_path)\n",
    "    _corpus = []\n",
    "    for filename in filenames:\n",
    "        with open(corpus_path+r\"/\"+filename, 'r', encoding='gbk') as f:\n",
    "            data = f.readlines()\n",
    "            for index in range(len(data)):\n",
    "                data[index] = str(data[index][:-1])\n",
    "            _corpus.extend(data)\n",
    "    return _corpus\n",
    "\n",
    "def exception_list(exception_path):\n",
    "    _exception_list = []\n",
    "    with open(exception_path, 'r', encoding='utf-8') as f:\n",
    "        data = f.readlines()\n",
    "        for index in range(len(data)):\n",
    "            data[index] = str(data[index][:-1])\n",
    "        _exception_list.extend(data)\n",
    "    return _exception_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "json_path = r\"/Users/lzd/Desktop/workspace/ZhongYiPapers/papers_lzd.json\"\n",
    "exception_path = r\"/Users/lzd/Desktop/workspace/ZhongYiPapers/词汇库/不包含的词汇.txt\"\n",
    "corpus_path = r\"/Users/lzd/Desktop/workspace/ZhongYiPapers/词汇库/处理完成\"\n",
    "jsons = open_json(json_path)\n",
    "my_corpus = corpus(corpus_path)\n",
    "my_exception_list = exception_list(exception_path)\n",
    "num = 0\n",
    "for item in jsons:\n",
    "    if \"中医\" in item[\"title\"] or \"中医\" in item[\"abstract\"]:\n",
    "        if \"中医\" not in jieba.lcut(item[\"title\"]) and \"中医\" not in jieba.lcut(item[\"abstract\"]):\n",
    "            continue\n",
    "        num +=1\n",
    "        # print(item[\"title\"], \"中医\")\n",
    "        continue\n",
    "    if \"中药\" in item[\"title\"] or \"中药\" in item[\"abstract\"]:\n",
    "        if \"中药\" not in jieba.lcut(item[\"title\"]) and \"中药\" not in jieba.lcut(item[\"abstract\"]):\n",
    "            continue\n",
    "        num +=1\n",
    "        # print(item[\"title\"], \"中药\")\n",
    "        continue\n",
    "    for word in my_corpus:\n",
    "        if word in item[\"title\"] or word in item[\"abstract\"]:\n",
    "            if word in my_exception_list:\n",
    "                continue\n",
    "            num +=1\n",
    "            # print(word, item[\"title\"])\n",
    "            break\n",
    "    print(item[\"title\"])\n",
    "num, len(jsons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:/vscode_workspace/ZhongYiPapers/database/分省份数据/result2010.json ： (25094, 15114)\n",
      "D:/vscode_workspace/ZhongYiPapers/database/分省份数据/result2011.json ： (27254, 16458)\n",
      "D:/vscode_workspace/ZhongYiPapers/database/分省份数据/result2012.json ： (29984, 18071)\n",
      "D:/vscode_workspace/ZhongYiPapers/database/分省份数据/result2013.json ： (32436, 19828)\n",
      "D:/vscode_workspace/ZhongYiPapers/database/分省份数据/result2014.json ： (33639, 20577)\n",
      "D:/vscode_workspace/ZhongYiPapers/database/分省份数据/result2015.json ： (32814, 19930)\n",
      "D:/vscode_workspace/ZhongYiPapers/database/分省份数据/result2016.json ： (32418, 19957)\n",
      "D:/vscode_workspace/ZhongYiPapers/database/分省份数据/result2017.json ： (32120, 19852)\n",
      "D:/vscode_workspace/ZhongYiPapers/database/分省份数据/result2018.json ： (31620, 19257)\n",
      "D:/vscode_workspace/ZhongYiPapers/database/分省份数据/result2019.json ： (32706, 19887)\n",
      "D:/vscode_workspace/ZhongYiPapers/database/分省份数据/result2020.json ： (34058, 20691)\n",
      "D:/vscode_workspace/ZhongYiPapers/database/分省份数据/result2021.json ： (34075, 20811)\n",
      "D:/vscode_workspace/ZhongYiPapers/database/分省份数据/result2022.json ： (33145, 20545)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import openpyxl\n",
    "import jieba\n",
    "import re\n",
    "import chardet\n",
    "'''获取词频，获取字频，给词频和字频附带分类个数'''\n",
    "words = {}\n",
    "characters = {}\n",
    "\n",
    "def open_json(filename):\n",
    "    with open(filename, 'r',encoding='utf-8') as f:\n",
    "        existing_data = json.load(f)\n",
    "        return existing_data\n",
    "\n",
    "def corpus(corpus_path):\n",
    "    filenames = os.listdir(corpus_path)\n",
    "    _corpus = []\n",
    "    for filename in filenames:\n",
    "        try:\n",
    "            with open(corpus_path+r\"/\"+filename, 'r', encoding='gbk') as f:\n",
    "                data = f.readlines()\n",
    "                for index in range(len(data)):\n",
    "                    data[index] = str(data[index][:-1])\n",
    "                _corpus.extend(data)\n",
    "        except:\n",
    "            with open(corpus_path+r\"/\"+filename, 'r', encoding='utf-8') as f:\n",
    "                data = f.readlines()\n",
    "                for index in range(len(data)):\n",
    "                    data[index] = str(data[index][:-1])\n",
    "                _corpus.extend(data)\n",
    "    return _corpus\n",
    "\n",
    "def exception_list(exception_path):\n",
    "    _exception_list = []\n",
    "    with open(exception_path, 'r', encoding='utf-8') as f:\n",
    "        data = f.readlines()\n",
    "        for index in range(len(data)):\n",
    "            data[index] = str(data[index][:-1])\n",
    "        _exception_list.extend(data)\n",
    "    return _exception_list\n",
    "\n",
    "def valid(strs):\n",
    "    #检验是否全是中文字符\n",
    "    # strs.encode(encoding=\"utf-8\")\n",
    "    if '，' in strs or '。' in strs:\n",
    "        return False\n",
    "    for _char in strs:\n",
    "        if not '\\u4e00' <= _char <= '\\u9fa5':\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def word_insert(word, zhongyi):\n",
    "    # 对words字典中的对应key进行value的增加\n",
    "    if word in words:\n",
    "        words[word] = [words[word][0]+1, words[word][1]+zhongyi]\n",
    "    else:\n",
    "        words[word] = [1, 0+zhongyi]\n",
    "\n",
    "def character_insert(character, zhongyi):\n",
    "    if character in characters:\n",
    "        characters[character] = [characters[character][0]+1, characters[character][1]+zhongyi]\n",
    "    else:\n",
    "        characters[character] = [1, 0+zhongyi]\n",
    "\n",
    "def split(item, zhongyi):\n",
    "    item_words = jieba.lcut(item[\"title\"])\n",
    "    for item_word in item_words:\n",
    "        if valid(item_word):\n",
    "            word_insert(item_word, zhongyi)\n",
    "    for cha in item[\"title\"]:\n",
    "        if valid(cha):\n",
    "            character_insert(cha, zhongyi)\n",
    "    item_words = jieba.lcut(item[\"abstract\"])\n",
    "    for item_word in item_words:\n",
    "        if valid(item_word):\n",
    "            word_insert(item_word, zhongyi)\n",
    "    for cha in item[\"abstract\"]:\n",
    "        if valid(cha):\n",
    "            character_insert(cha, zhongyi)\n",
    "\n",
    "def save_to(entry, filename):\n",
    "    try:\n",
    "        # utf-8将一个汉字编码为3个字节,gbk将一个汉字编码为2个字节,\n",
    "        with open(filename, 'w',encoding='utf-8') as new_f:  # 重新写入\n",
    "            json.dump(entry, new_f,ensure_ascii=False,indent=4) # 最后一个参数会保证dump之后的结果所有的字符都能被ascii表示\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        print('Make sure the json file is in valid format,[{},{}...]')\n",
    "\n",
    "\n",
    "def wordFrequency(json_path):\n",
    "    exception_path = r\"D:/vscode_workspace/ZhongYiPapers/词汇库/不包含的词汇.txt\"\n",
    "    corpus_path = r\"D:/vscode_workspace/ZhongYiPapers/词汇库/处理完成\"\n",
    "    jsons = open_json(json_path)\n",
    "    my_corpus = corpus(corpus_path)\n",
    "    my_exception_list = exception_list(exception_path)\n",
    "    num = 0\n",
    "    for item in jsons:\n",
    "        if \"title\" not in item:\n",
    "            continue\n",
    "        if \"中医\" in item[\"title\"] or \"中医\" in item[\"abstract\"]:\n",
    "            if \"中医\" not in jieba.lcut(item[\"title\"]) and \"中医\" not in jieba.lcut(item[\"abstract\"]):\n",
    "                continue\n",
    "            num +=1\n",
    "            split(item, True)\n",
    "            continue\n",
    "        if \"中药\" in item[\"title\"] or \"中药\" in item[\"abstract\"]:\n",
    "            if \"中药\" not in jieba.lcut(item[\"title\"]) and \"中药\" not in jieba.lcut(item[\"abstract\"]):\n",
    "                continue\n",
    "            num +=1\n",
    "            split(item, True)\n",
    "            continue\n",
    "        for word in my_corpus:\n",
    "            if word in item[\"title\"] or word in item[\"abstract\"]:\n",
    "                if word in my_exception_list:\n",
    "                    continue\n",
    "                num +=1\n",
    "                split(item, True)\n",
    "                break\n",
    "        split(item, False)\n",
    "    print(json_path, \"：\", (len(jsons), num))\n",
    "\n",
    "def save_Frequency(database_path):\n",
    "    # 保存到json\n",
    "    save_to(words, database_path + '/' + r\"words.json\")\n",
    "    save_to(characters, database_path +'/' + r\"characters.json\")\n",
    "    # 保存到excel\n",
    "    workbook = openpyxl.Workbook()\n",
    "    word_sheet = workbook.active\n",
    "    word_sheet.title = 'words'\n",
    "    for word in words.keys():\n",
    "        word_sheet.append([word, words[word][0], words[word][1]])\n",
    "    character_sheet = workbook.create_sheet(index=1, title=\"characters\")\n",
    "    for character in characters.keys():\n",
    "        character_sheet.append([character, characters[character][0], characters[character][1]])\n",
    "    workbook.save(database_path + \"/\" + 'frequency.xlsx')\n",
    "\n",
    "def main():\n",
    "    json_path = r\"D:/vscode_workspace/ZhongYiPapers/database/分省份数据\"\n",
    "    # json_path = r\"D:/vscode_workspace/ZhongYiPapers/database/TMP\"\n",
    "    database_path = r\"D:/vscode_workspace/ZhongYiPapers/database/12_23新处理\"\n",
    "    json_names = os.listdir(json_path)\n",
    "    for json_name in json_names:\n",
    "        wordFrequency(json_path+'/'+json_name)\n",
    "    save_Frequency(database_path)\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, openpyxl\n",
    "with open(r\"D:/vscode_workspace/ZhongYiPapers/database/12_22新处理/words.json\", 'r',encoding='utf-8') as f:\n",
    "    words_json = json.load(f)\n",
    "with open(r\"D:/vscode_workspace/ZhongYiPapers/词汇库/common_chinese_characters.json\", 'r',encoding='utf-8') as f:\n",
    "    common_chinese_characters = json.load(f)\n",
    "# 保存到excel\n",
    "workbook = openpyxl.Workbook()\n",
    "word_sheet = workbook.active\n",
    "word_sheet.title = 'words'\n",
    "qualified = False\n",
    "for word in words_json.keys():\n",
    "    qualified = False\n",
    "    for cha in word:\n",
    "        if cha not in common_chinese_characters:\n",
    "            qualified = True\n",
    "            break\n",
    "    if not qualified:\n",
    "        continue\n",
    "    word_sheet.append([word, words_json[word][0], words_json[word][1]])\n",
    "workbook.save(r\"D:/vscode_workspace/ZhongYiPapers/database/12_23新处理\" + \"/\" + 'frequency.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, '中医药', ['针灸', '经络', '穴位', '艾灸', '中西医', '中医药', '中西医结合'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "tmp = jieba.lcut(r\"中医药诊疗干预亚健康状态的研究进展\")\n",
    "corpus = []\n",
    "with open(r\"D:\\vscode_workspace\\ZhongYiPapers\\词汇库\\处理完成\\新纳入.txt\", 'r', encoding='utf-8') as f:\n",
    "# with open(r\"D:\\vscode_workspace\\ZhongYiPapers\\词汇库\\处理完成\\中医症候.txt\", 'r', encoding='gbk') as f:\n",
    "    data = f.readlines()\n",
    "    for index in range(len(data)):\n",
    "        data[index] = data[index].strip()\n",
    "        # data[index] = str(data[index][:-1])\n",
    "    corpus.extend(data)\n",
    "tmp[0] in corpus, tmp[0], corpus"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
