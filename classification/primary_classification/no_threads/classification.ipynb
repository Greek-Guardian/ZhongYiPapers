{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D://vscode_workspace//ZhongYiPapers//database//分省份数据/papers_lzd.json ： (44146, 31755)\n",
      "D://vscode_workspace//ZhongYiPapers//database//分省份数据/result2010.json ： (25094, 17471)\n",
      "D://vscode_workspace//ZhongYiPapers//database//分省份数据/result2011.json ： (27254, 19122)\n",
      "D://vscode_workspace//ZhongYiPapers//database//分省份数据/result2012.json ： (29984, 20804)\n",
      "D://vscode_workspace//ZhongYiPapers//database//分省份数据/result2013.json ： (32436, 22725)\n",
      "D://vscode_workspace//ZhongYiPapers//database//分省份数据/result2014.json ： (33639, 23713)\n",
      "D://vscode_workspace//ZhongYiPapers//database//分省份数据/result2015.json ： (32814, 22993)\n",
      "D://vscode_workspace//ZhongYiPapers//database//分省份数据/result2016.json ： (32418, 23068)\n",
      "D://vscode_workspace//ZhongYiPapers//database//分省份数据/result2017.json ： (32120, 23007)\n",
      "D://vscode_workspace//ZhongYiPapers//database//分省份数据/result2018.json ： (31620, 22458)\n",
      "D://vscode_workspace//ZhongYiPapers//database//分省份数据/result2019.json ： (32706, 23283)\n",
      "D://vscode_workspace//ZhongYiPapers//database//分省份数据/result2020.json ： (34058, 24702)\n",
      "D://vscode_workspace//ZhongYiPapers//database//分省份数据/result2021.json ： (34075, 24906)\n",
      "D://vscode_workspace//ZhongYiPapers//database//分省份数据/result2022.json ： (33145, 24793)\n",
      "+----------------+-----------------+--------------------+\n",
      "| Total item num | Total valid num |       Ratio        |\n",
      "+----------------+-----------------+--------------------+\n",
      "|     455509     |      324800     | 0.7130484798324511 |\n",
      "+----------------+-----------------+--------------------+\n",
      "+--------------------+-----------------------+----------------------+\n",
      "|      Variant       |          Mean         |         Var          |\n",
      "+--------------------+-----------------------+----------------------+\n",
      "|       Words        |  0.057681644447174364 | 0.00564139280873183  |\n",
      "|    Valid words     |  0.06848067174656476  | 0.005185928147432478 |\n",
      "|   Invalid words    |  0.030844372710916185 | 0.00576323813938946  |\n",
      "|     Characters     | -0.022154351483292332 | 0.013293436676444998 |\n",
      "|  Valid characters  | -0.009345376028651108 | 0.011945630316602304 |\n",
      "| Invalid characters |  -0.05398665872810066 | 0.01522191124206652  |\n",
      "+--------------------+-----------------------+----------------------+\n",
      "+------------------+--------------------+\n",
      "|     Function     |     Time cost      |\n",
      "+------------------+--------------------+\n",
      "| split_and_insert | 897.1053261756897  |\n",
      "|     scoring      |  780.257128238678  |\n",
      "|   item_process   | 1683.4668626785278 |\n",
      "+------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import openpyxl\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import time\n",
    "from prettytable import PrettyTable\n",
    "# import inspect, re\n",
    "import numpy as np\n",
    "'''获取词频，获取字频，给词频和字频附带分类个数'''\n",
    "words = {}\n",
    "characters = {}\n",
    "valid_papers = {}\n",
    "invalid_papers = {}\n",
    "# 得分\n",
    "valid_words_score_list = []\n",
    "valid_characters_score_list = []\n",
    "invalid_words_score_list = []\n",
    "invalid_characters_score_list = []\n",
    "# 函数计时器\n",
    "timer_split_and_insert_count = 0\n",
    "timer_scoring_count = 0\n",
    "timer_item_process_count = 0\n",
    "\n",
    "# def varname(p):\n",
    "# # 获取变量名\n",
    "#     for line in inspect.getframeinfo(inspect.currentframe().f_back)[3]:\n",
    "#         m = re.search(r'\\bvarname\\s*\\(\\s*([A-Za-z_][A-Za-z0-9_]*)\\s*\\)', line)\n",
    "#     if m:\n",
    "#         return m.group(1)\n",
    "\n",
    "def timer_split_and_insert(func):\n",
    "    def func_in(*s,**gs):\n",
    "        start_time = time.time()\n",
    "        _res_ = func(*s,**gs)\n",
    "        end_time = time.time()\n",
    "        global timer_split_and_insert_count\n",
    "        timer_split_and_insert_count += end_time - start_time\n",
    "        return  _res_\n",
    "    return  func_in\n",
    "\n",
    "def timer_scoring(func):\n",
    "    def func_in(*s,**gs):\n",
    "        start_time = time.time()\n",
    "        _res_ = func(*s,**gs)\n",
    "        end_time = time.time()\n",
    "        global timer_scoring_count\n",
    "        timer_scoring_count += end_time - start_time\n",
    "        return  _res_\n",
    "    return  func_in\n",
    "\n",
    "def timer_item_process(func):\n",
    "    def func_in(*s,**gs):\n",
    "        start_time = time.time()\n",
    "        _res_ = func(*s,**gs)\n",
    "        end_time = time.time()\n",
    "        global timer_item_process_count\n",
    "        timer_item_process_count += end_time - start_time\n",
    "        return  _res_\n",
    "    return  func_in\n",
    "\n",
    "def corpus(corpus_path, exception_path):\n",
    "    def exception_list(exception_path):\n",
    "        _exception_list_ = []\n",
    "        with open(exception_path, 'r', encoding='utf-8') as f:\n",
    "            data = f.readlines()\n",
    "            for index in range(len(data)):\n",
    "                data[index] = str(data[index][:-1])\n",
    "            _exception_list_.extend(data)\n",
    "        return _exception_list_\n",
    "\n",
    "    filenames = os.listdir(corpus_path)\n",
    "    _corpus_ = []\n",
    "    for filename in filenames:\n",
    "        try:\n",
    "            with open(corpus_path+r\"/\"+filename, 'r', encoding='gbk') as f:\n",
    "                data = f.readlines()\n",
    "                for index in range(len(data)):\n",
    "                    data[index] = data[index].strip()\n",
    "                _corpus_.extend(data)\n",
    "        except:\n",
    "            with open(corpus_path+r\"/\"+filename, 'r', encoding='utf-8') as f:\n",
    "                data = f.readlines()\n",
    "                for index in range(len(data)):\n",
    "                    data[index] = data[index].strip()\n",
    "                _corpus_.extend(data)\n",
    "    # 删除 exception list 中的单词\n",
    "    _corpus_exception_list_ = exception_list(exception_path)\n",
    "    for _corpus_exception_word_ in _corpus_exception_list_:\n",
    "        try:\n",
    "            _corpus_.__delitem__(_corpus_.index(_corpus_exception_word_))\n",
    "        except:\n",
    "            pass\n",
    "    return _corpus_\n",
    "\n",
    "@timer_split_and_insert\n",
    "def split_and_insert(item, ZhongYi_or_not):\n",
    "    def valid(strs):\n",
    "        #检验是否全是中文字符\n",
    "        for _char in strs:\n",
    "            if not '\\u4e00' <= _char <= '\\u9fa5':\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def word_insert(word, ZhongYi_or_not):\n",
    "        # 对words字典中的对应key进行value的增加\n",
    "        if word in words:\n",
    "            words[word] = [words[word][0]+1, words[word][1]+ZhongYi_or_not]\n",
    "        else:\n",
    "            words[word] = [1, 0+ZhongYi_or_not]\n",
    "\n",
    "    def character_insert(character, ZhongYi_or_not):\n",
    "        if character in characters:\n",
    "            characters[character] = [characters[character][0]+1, characters[character][1]+ZhongYi_or_not]\n",
    "        else:\n",
    "            characters[character] = [1, 0+ZhongYi_or_not]\n",
    "\n",
    "    json_keys = [\"title\", \"abstract\", \"keywords\"]\n",
    "    for json_key in json_keys:\n",
    "        item_words = jieba.lcut(item[json_key])\n",
    "        for item_word in item_words:\n",
    "            if valid(item_word):\n",
    "                word_insert(item_word, ZhongYi_or_not)\n",
    "        for cha in item[json_key]:\n",
    "            if valid(cha):\n",
    "                character_insert(cha, ZhongYi_or_not)\n",
    "\n",
    "@timer_scoring\n",
    "def scoring(item, excel_words, excel_characters, common_chinese_characters, score_method = 'words'):\n",
    "    item_text = item[\"title\"] + \"。\" + item[\"abstract\"] + \"。\" + item[\"keywords\"]\n",
    "    score = 0\n",
    "    common_chinese_characters_weight = 1/5\n",
    "    common_chinese_words_weight = 1/3\n",
    "    if score_method == 'words':\n",
    "        # 按单词打分\n",
    "        item_text_WordsOrCharacters = jieba.lcut(item_text)\n",
    "        prob_dict = excel_words\n",
    "    else:\n",
    "        # 按字符打分\n",
    "        item_text_WordsOrCharacters = item_text\n",
    "        prob_dict = excel_characters\n",
    "\n",
    "    for item_text_WordOrCharacter in item_text_WordsOrCharacters:\n",
    "        # 舍弃特殊字符（标点符号等）。若单词或字符为中文//英文//数字，isalnum()函数则为True\n",
    "        if item_text_WordOrCharacter.isalnum():\n",
    "            # 判断是否为中文\n",
    "            is_Chinese = True\n",
    "            for _char in item_text_WordOrCharacter:\n",
    "                if not '\\u4e00' <= _char <= '\\u9fa5':\n",
    "                    is_Chinese = False\n",
    "                    break\n",
    "            # 若单词或字符为英文或数字，则减1分\n",
    "            if not is_Chinese:\n",
    "                score -= 1\n",
    "                continue\n",
    "            # 判断是否包含不常用汉字\n",
    "            is_common_chinese = True\n",
    "            for _char in item_text_WordOrCharacter:\n",
    "                if _char not in common_chinese_characters:\n",
    "                    is_common_chinese = False\n",
    "                    break\n",
    "            try:\n",
    "                if is_common_chinese:\n",
    "                    # 若为常用中文单词，则将单词概率的1/3或1/5作为分数。\n",
    "                    score += prob_dict[item_text_WordOrCharacter]['概率']*\\\n",
    "                        (common_chinese_characters_weight*(1-(score_method=='words'))+\\\n",
    "                         common_chinese_words_weight*(score_method=='words'))\n",
    "                else:\n",
    "                    # 若单词包含不常用汉字字符，则将概率作为分数\n",
    "                    score += prob_dict[item_text_WordOrCharacter]['概率']\n",
    "            except:\n",
    "                # 舍弃excel中不包含的单词\n",
    "                pass\n",
    "    score /= len(item_text_WordsOrCharacters)\n",
    "    return score\n",
    "\n",
    "def wordFrequency(json_path, zhongyi_corpus, excel_words, excel_characters, common_chinese_characters):\n",
    "    # 读取文章json\n",
    "    with open(json_path, 'r',encoding='utf-8') as f:\n",
    "        items_json = json.load(f)\n",
    "    # 优先处理的条目\n",
    "    most_common_word_list = [\"中医\", \"中药\", \"中医药\", \"中西医\"]\n",
    "    # 中医文章的数量\n",
    "    valid_num = 0\n",
    "\n",
    "    def check_and_checkGrammar(word, item_text):\n",
    "        if word in item_text:\n",
    "            if word in jieba.lcut(item_text):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    @timer_item_process\n",
    "    def item_process(item, identify_word, valid_or_not):\n",
    "        split_and_insert(item, valid_or_not)\n",
    "        words_score = scoring(item, excel_words, excel_characters, common_chinese_characters, score_method='words')\n",
    "        characters_score = scoring(item, excel_words, excel_characters, common_chinese_characters, score_method='characters')\n",
    "        if valid_or_not:\n",
    "            valid_papers[item[\"title\"]] = {\n",
    "                \"abstract\": item[\"abstract\"],\n",
    "                \"keywords\": item[\"keywords\"],\n",
    "                \"identify_word\": identify_word,\n",
    "                \"words_score\": words_score,\n",
    "                \"characters_score\": characters_score,\n",
    "                \"valid\": valid_or_not,\n",
    "            }\n",
    "            valid_words_score_list.append(words_score)\n",
    "            valid_characters_score_list.append(characters_score)\n",
    "        else:\n",
    "            invalid_papers[item[\"title\"]] = {\n",
    "                \"abstract\": item[\"abstract\"],\n",
    "                \"keywords\": item[\"keywords\"],\n",
    "                \"identify_word\": identify_word,\n",
    "                \"words_score\": words_score,\n",
    "                \"characters_score\": characters_score,\n",
    "                \"valid\": valid_or_not,\n",
    "            }\n",
    "            invalid_words_score_list.append(words_score)\n",
    "            invalid_characters_score_list.append(characters_score)\n",
    "\n",
    "    for item in items_json:\n",
    "        # 跳过非条目项\n",
    "        if \"title\" not in item:\n",
    "            continue\n",
    "        item_text = item[\"title\"] + \"。\" + item[\"abstract\"] + \"。\" + item[\"keywords\"]\n",
    "        continue_tag = False\n",
    "        # 处理最常见的单词\n",
    "        for most_common_word in most_common_word_list:\n",
    "            if check_and_checkGrammar(most_common_word, item_text):\n",
    "                valid_num += 1\n",
    "                item_process(item, most_common_word, True)\n",
    "                continue_tag = True\n",
    "                break\n",
    "        if continue_tag:\n",
    "            continue\n",
    "        # 遍历整个corpus，寻找item中出现的单词\n",
    "        for word in zhongyi_corpus:\n",
    "            if word in item_text:\n",
    "                valid_num += 1\n",
    "                item_process(item, word, True)\n",
    "                continue_tag = True\n",
    "                break\n",
    "        if continue_tag:\n",
    "            continue\n",
    "        # 本篇文章未找到中医关键词\n",
    "        item_process(item, \"\", False)\n",
    "    print(json_path, \"：\", (len(items_json), valid_num))\n",
    "    return len(items_json), valid_num\n",
    "\n",
    "def save_Frequency(database_path):\n",
    "    def save_to(entry, filename):\n",
    "        try:\n",
    "            # utf-8将一个汉字编码为3个字节,gbk将一个汉字编码为2个字节,\n",
    "            with open(filename, 'w',encoding='utf-8') as new_f:  # 重新写入\n",
    "                json.dump(entry, new_f,ensure_ascii=False,indent=4) # 最后一个参数会保证dump之后的结果所有的字符都能被ascii表示\n",
    "        except json.decoder.JSONDecodeError:\n",
    "            print('Make sure the json file is in valid format,[{},{}...]')\n",
    "    # 保存到json\n",
    "    save_to(words, database_path + '/' + r\"words.json\")\n",
    "    save_to(characters, database_path +'/' + r\"characters.json\")\n",
    "    # 保存到excel\n",
    "    workbook = openpyxl.Workbook()\n",
    "    word_sheet = workbook.active\n",
    "    word_sheet.title = 'words'\n",
    "    for word in words.keys():\n",
    "        word_sheet.append([word, words[word][0], words[word][1]])\n",
    "    character_sheet = workbook.create_sheet(index=1, title=\"characters\")\n",
    "    for character in characters.keys():\n",
    "        character_sheet.append([character, characters[character][0], characters[character][1]])\n",
    "    workbook.save(database_path + \"/\" + 'frequency.xlsx')\n",
    "\n",
    "def save_Json(database_path):\n",
    "    def save_to(entry, filename):\n",
    "        try:\n",
    "            # utf-8将一个汉字编码为3个字节,gbk将一个汉字编码为2个字节,\n",
    "            with open(filename, 'w',encoding='utf-8') as new_f:  # 重新写入\n",
    "                json.dump(entry, new_f,ensure_ascii=False,indent=4) # 最后一个参数会保证dump之后的结果所有的字符都能被ascii表示\n",
    "        except json.decoder.JSONDecodeError:\n",
    "            print('Make sure the json file is in valid format,[{},{}...]')\n",
    "    save_to(valid_papers, database_path + '/' + 'valid_papers.json')\n",
    "    save_to(invalid_papers, database_path + '/' + 'invalid_papers.json')\n",
    "\n",
    "def main():\n",
    "    json_path = r\"D://vscode_workspace//ZhongYiPapers//database//分省份数据\"\n",
    "    # json_path = r\"D://vscode_workspace//ZhongYiPapers//database//TMP\"\n",
    "    save_path = r\"D://vscode_workspace//ZhongYiPapers//database//12_24新处理\"\n",
    "    common_chinese_characters_path = r\"D://vscode_workspace//ZhongYiPapers//词汇库//common_chinese_characters.json\"\n",
    "    prob_excel_path = r\"D://vscode_workspace//ZhongYiPapers//database//12_23新处理//frequency.xlsx\"\n",
    "    exception_path = r\"D://vscode_workspace//ZhongYiPapers//词汇库//不包含的词汇.txt\"\n",
    "    corpus_path = r\"D://vscode_workspace//ZhongYiPapers//词汇库//处理完成\"\n",
    "    # 读取常用汉字\n",
    "    with open(common_chinese_characters_path, 'r',encoding='utf-8') as f:\n",
    "        common_chinese_characters = json.load(f)\n",
    "        common_chinese_characters = set(common_chinese_characters)\n",
    "    # 读取词频\n",
    "    excel_words = pd.DataFrame(pd.read_excel(prob_excel_path, sheet_name='words', header=0, index_col=0))\n",
    "    excel_words = excel_words.to_dict('index')\n",
    "    excel_characters = pd.DataFrame(pd.read_excel(prob_excel_path, sheet_name='characters', header=0, index_col=0))\n",
    "    excel_characters = excel_characters.to_dict('index')\n",
    "    # 读取中医词库\n",
    "    zhongyi_corpus = corpus(corpus_path, exception_path)\n",
    "    # 读取需要处理的json列表\n",
    "    json_names = os.listdir(json_path)\n",
    "    # 依次处理json\n",
    "    total_json_num = 0\n",
    "    total_valid_num = 0\n",
    "    for json_name in json_names:\n",
    "        json_len, valid_num = wordFrequency(json_path+'/'+json_name, zhongyi_corpus, excel_words, excel_characters, common_chinese_characters)\n",
    "        total_json_num += json_len\n",
    "        total_valid_num += valid_num\n",
    "    save_Frequency(save_path)\n",
    "    save_Json(save_path)\n",
    "\n",
    "    table = PrettyTable(['Total item num','Total valid num','Ratio'])\n",
    "    table.add_row([total_json_num, total_valid_num, total_valid_num/total_json_num])\n",
    "    print(table)\n",
    "\n",
    "    words_score_list = valid_words_score_list + invalid_words_score_list\n",
    "    characters_score_list = valid_characters_score_list + invalid_characters_score_list\n",
    "    table = PrettyTable(['Variant','Mean','Var'])\n",
    "    table.add_row([\"Words\", np.mean(words_score_list), np.var(words_score_list)])\n",
    "    table.add_row([\"Valid words\", np.mean(valid_words_score_list), np.var(valid_words_score_list)])\n",
    "    table.add_row([\"Invalid words\", np.mean(invalid_words_score_list), np.var(invalid_words_score_list)])\n",
    "    table.add_row([\"Characters\", np.mean(characters_score_list), np.var(characters_score_list)])\n",
    "    table.add_row([\"Valid characters\", np.mean(valid_characters_score_list), np.var(valid_characters_score_list)])\n",
    "    table.add_row([\"Invalid characters\", np.mean(invalid_characters_score_list), np.var(invalid_characters_score_list)])\n",
    "    print(table)\n",
    "\n",
    "    table = PrettyTable(['Function','Time cost'])\n",
    "    table.add_row([\"split_and_insert\", timer_split_and_insert_count])\n",
    "    table.add_row([\"scoring\", timer_scoring_count])\n",
    "    table.add_row([\"item_process\", timer_item_process_count])\n",
    "    print(table)\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
